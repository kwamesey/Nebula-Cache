# NebulaCache â˜ï¸ðŸŒ€  
**A distributed cache that refuses to die.**
I built NebulaCache because I wanted to learn how real distributed caches (think Redis Cluster, Memcached, Dynamo-style systems) handle **scaling**, **rebalancing**, and **node failure** â€” but I also wanted it to be fun to run from the terminal.
NebulaCache is a Python-based, terminal-first, simulated distributed in-memory cache. It uses:
- ðŸŒ€ **Consistent hashing** â€” data is evenly spread across nodes.  
- â¤ï¸ **Replication** â€” one dead node doesnâ€™t mean heartbreak.  
- ðŸ“¡ **Gossip protocol** â€” nodes â€œtalkâ€ behind each otherâ€™s backs about whoâ€™s alive.  
- âš¡ **Dynamic scaling** â€” add or remove nodes while everything stays balanced.  
- ðŸŽ¨ **Rich CLI output** â€” colorful terminal tables because logs deserve vibes too.
The vibe: *â€œwhat if a cache had trust issues and over-communicated?â€*
---
## ðŸ’» Features
- Horizontal scalability via consistent hashing  
- Fault-tolerance through replication  
- Self-healing simulation with node failures and recovery  
- Interactive CLI (`put`, `get`, `add-node`, `fail-node`, `show-nodes`, `where`)  
- No servers, no databases â€” just pure distributed chaos in your terminal  
---
## âš™ï¸ Quickstart
create and activate a venv (optional)
python -m venv .venv && source .venv/bin/activate
install deps
pip install -r requirements.txt
start a cluster with 3 nodes
python -m cache.cli --nodes 3
Inside the prompt:
> put user:1 "kwame"
> get user:1
> add-node
> show-nodes
> fail-node node-2
> heal-node node-2
> where user:1
> exit
Youâ€™ll watch the ring rebalance and data move to healthy nodes â€” like ants rebuilding a colony after I accidentally kicked it (the ants are Docker containers, for legal reasons).
---
## ðŸ§  Architecture Overview
User CLI  
â”‚  
â”œâ”€â”€ CacheCluster (manages nodes & hashing)  
â”‚   â”œâ”€â”€ ConsistentHashRing  â†’ maps keys â†’ nodes  
â”‚   â”œâ”€â”€ GossipManager       â†’ tracks node health  
â”‚   â””â”€â”€ CacheNode           â†’ stores key/value pairs  
â”‚  
â””â”€â”€ Rich-powered CLI UI     â†’ displays node health + data placement  
Each key is stored on multiple nodes (replication factor configurable). If a node fails, the next node in the ring instantly takes over responsibility for that range of keys.  
---
## ðŸ§° Tech Stack
- **Language:** Python 3.11  
- **Libraries:** `asyncio`, `rich`, `argparse`  
- **Infra:** optional Docker Compose for one-command clusters  
---
## ðŸ§© Example Commands
Add a node to the ring  
> add-node  
Kill a node  
> fail-node node-3  
Heal it again  
> heal-node node-3  
Show who owns a key  
> where user:1  
---
## â˜• Motivation
Redis memes are funny until your cache actually dies. So I built my own â€” one that scales, heals, gossips, and looks good doing it.  
---
## ðŸ§¾ License
MIT â€” fork it, extend it, or just run it to feel like youâ€™re managing a tiny cloud. PRs welcome.
